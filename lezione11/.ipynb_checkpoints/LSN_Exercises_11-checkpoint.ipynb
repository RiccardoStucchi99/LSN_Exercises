{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\"> Numerical Simulation Laboratory </span>\n",
    "## <span style=\"color:brown\"> Python Exercise 11 </span>\n",
    "## <span style=\"color:orange\"> Keras - Neural Network regression </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview \n",
    "\n",
    "In this notebook our task will be to perform machine learning regression on noisy data with a Neural Network (NN).\n",
    "\n",
    "We will explore how the ability to fit depends on the structure of the NN. The goal is also to build intuition about why prediction is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Prediction Problem\n",
    "\n",
    "Consider a probabilistic process that gives rise to labeled data $(x,y)$. The data is generated by drawing samples from the equation\n",
    "\n",
    "$$\n",
    "    y_i= f(x_i) + \\eta_i,\n",
    "$$\n",
    "\n",
    "where $f(x_i)$ is some fixed, but (possibly unknown) function, and $\\eta_i$ is a Gaussian, uncorrelate noise variable such that\n",
    "\n",
    "$$\n",
    "\\langle \\eta_i \\rangle=0 \\\\\n",
    "\\langle \\eta_i \\eta_j \\rangle = \\delta_{ij} \\sigma\n",
    "$$\n",
    "\n",
    "We will refer to the $f(x_i)$ as the **true features** used to generate the data. \n",
    "\n",
    "To make predictions, we will consider a NN that depends on its parameters, weights and biases. The functions that the NN can model respresent the **model class** that we are using to try to model the data and make predictions.\n",
    "\n",
    "To learn the parameters of the NN, we will train our models on a **training data set** and then test the effectiveness of the NN on a *different* dataset, the **validation data set**. The reason we must divide our data into a training and test dataset is that the point of machine learning is to make accurate predictions about new data we have not seen.\n",
    "\n",
    "To measure our ability to predict, we will learn our parameters by fitting our training dataset and then making predictions on our test data set. One common measure of predictive  performance of our algorithm is to compare the predictions,$\\{y_j^\\mathrm{pred}\\}$, to the true values $\\{y_j\\}$. A commonly employed measure for this is the sum of the mean square-error (MSE) on the test set:\n",
    "$$\n",
    "MSE= \\frac{1}{N_\\mathrm{test}}\\sum_{j=1}^{N_\\mathrm{test}} (y_j^\\mathrm{pred}-y_j)^2\n",
    "$$\n",
    "\n",
    "We will try to get a qualitative picture by examining plots on validation and training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear fit\n",
    "\n",
    "We start by considering the very simple case:\n",
    "$$\n",
    "f(x)=2x+1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start defining the parameters of an ideal linear function which we are going to predict through a neural network regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target parameters of f(x) = m*x + b\n",
    "m = 2 # slope\n",
    "b = 1 # intersect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate a set of input data which will slightly deviate from our ideal behaviour using a random noise (that actually is set to zero):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate training inputs\n",
    "np.random.seed(0)\n",
    "x_train = np.random.uniform(-1, 1, 500)\n",
    "x_valid = np.random.uniform(-1, 1, 50)\n",
    "x_valid.sort()\n",
    "y_target = m * x_valid + b # ideal (target) linear function\n",
    "\n",
    "sigma = 0.0 # noise standard deviation, for the moment it is absent\n",
    "y_train = np.random.normal(m * x_train + b, sigma) # actual measures from which we want to guess regression parameters\n",
    "y_valid = np.random.normal(m * x_valid + b, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot validation and target dataset\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_valid, y_target, label='target')\n",
    "plt.scatter(x_valid, y_valid, color='r', label='validation data')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember how a single node of a neural network works, you can easily spot that **just a single neuron can make the job**. So let's start using a simple Sequential model with just one layer on one neuron only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose the NN model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(1, input_shape=(1,)))\n",
    "\n",
    "# compile the model choosing optimizer, loss and metrics objects\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get a summary of our composed model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to train our model, that is we feed the neuron with the set of training pair x, y_train from which the optimizer will find the best weights to minimize the Mean Square Error loss function (out linear regression function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.3548 - mse: 1.3548 - val_loss: 0.9420 - val_mse: 0.9420\n",
      "Epoch 2/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.8839 - mse: 0.8839 - val_loss: 0.6276 - val_mse: 0.6276\n",
      "Epoch 3/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6029 - mse: 0.6029 - val_loss: 0.4387 - val_mse: 0.4387\n",
      "Epoch 4/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4288 - mse: 0.4288 - val_loss: 0.3176 - val_mse: 0.3176\n",
      "Epoch 5/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3141 - mse: 0.3141 - val_loss: 0.2378 - val_mse: 0.2378\n",
      "Epoch 6/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.2366 - mse: 0.2366 - val_loss: 0.1825 - val_mse: 0.1825\n",
      "Epoch 7/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1818 - mse: 0.1818 - val_loss: 0.1427 - val_mse: 0.1427\n",
      "Epoch 8/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1420 - mse: 0.1420 - val_loss: 0.1130 - val_mse: 0.1130\n",
      "Epoch 9/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1122 - mse: 0.1122 - val_loss: 0.0899 - val_mse: 0.0899\n",
      "Epoch 10/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0890 - mse: 0.0890 - val_loss: 0.0718 - val_mse: 0.0718\n",
      "Epoch 11/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0709 - mse: 0.0709 - val_loss: 0.0577 - val_mse: 0.0577\n",
      "Epoch 12/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0568 - mse: 0.0568 - val_loss: 0.0463 - val_mse: 0.0463\n",
      "Epoch 13/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0455 - mse: 0.0455 - val_loss: 0.0372 - val_mse: 0.0372\n",
      "Epoch 14/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.0299 - val_mse: 0.0299\n",
      "Epoch 15/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0293 - mse: 0.0293 - val_loss: 0.0241 - val_mse: 0.0241\n",
      "Epoch 16/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0235 - mse: 0.0235 - val_loss: 0.0194 - val_mse: 0.0194\n",
      "Epoch 17/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.0156 - val_mse: 0.0156\n",
      "Epoch 18/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.0125 - val_mse: 0.0125\n",
      "Epoch 19/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.0101 - val_mse: 0.0101\n",
      "Epoch 20/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0081 - val_mse: 0.0081\n",
      "Epoch 21/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0065 - val_mse: 0.0065\n",
      "Epoch 22/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0052 - val_mse: 0.0052\n",
      "Epoch 23/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0042 - val_mse: 0.0042\n",
      "Epoch 24/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0034 - val_mse: 0.0034\n",
      "Epoch 25/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0027 - val_mse: 0.0027\n",
      "Epoch 26/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0022 - val_mse: 0.0022\n",
      "Epoch 27/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0018 - val_mse: 0.0018\n",
      "Epoch 28/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0014 - val_mse: 0.0014\n",
      "Epoch 29/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0011 - val_mse: 0.0011\n",
      "Epoch 30/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 9.1672e-04 - val_mse: 9.1672e-04\n",
      "Epoch 31/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 8.8910e-04 - mse: 8.8910e-04 - val_loss: 7.3638e-04 - val_mse: 7.3638e-04\n",
      "Epoch 32/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.1431e-04 - mse: 7.1431e-04 - val_loss: 5.9182e-04 - val_mse: 5.9182e-04\n",
      "Epoch 33/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.7386e-04 - mse: 5.7386e-04 - val_loss: 4.7574e-04 - val_mse: 4.7574e-04\n",
      "Epoch 34/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.6110e-04 - mse: 4.6110e-04 - val_loss: 3.8262e-04 - val_mse: 3.8262e-04\n",
      "Epoch 35/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7099e-04 - mse: 3.7099e-04 - val_loss: 3.0712e-04 - val_mse: 3.0712e-04\n",
      "Epoch 36/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.9776e-04 - mse: 2.9776e-04 - val_loss: 2.4697e-04 - val_mse: 2.4697e-04\n",
      "Epoch 37/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.3942e-04 - mse: 2.3942e-04 - val_loss: 1.9865e-04 - val_mse: 1.9865e-04\n",
      "Epoch 38/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.9262e-04 - mse: 1.9262e-04 - val_loss: 1.5971e-04 - val_mse: 1.5971e-04\n",
      "Epoch 39/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.5489e-04 - mse: 1.5489e-04 - val_loss: 1.2840e-04 - val_mse: 1.2840e-04\n",
      "Epoch 40/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2448e-04 - mse: 1.2448e-04 - val_loss: 1.0327e-04 - val_mse: 1.0327e-04\n",
      "Epoch 41/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0013e-04 - mse: 1.0013e-04 - val_loss: 8.2900e-05 - val_mse: 8.2900e-05\n",
      "Epoch 42/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.0370e-05 - mse: 8.0370e-05 - val_loss: 6.6694e-05 - val_mse: 6.6694e-05\n",
      "Epoch 43/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.4673e-05 - mse: 6.4673e-05 - val_loss: 5.3644e-05 - val_mse: 5.3644e-05\n",
      "Epoch 44/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.2015e-05 - mse: 5.2015e-05 - val_loss: 4.3138e-05 - val_mse: 4.3138e-05\n",
      "Epoch 45/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1862e-05 - mse: 4.1862e-05 - val_loss: 3.4689e-05 - val_mse: 3.4689e-05\n",
      "Epoch 46/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.3647e-05 - mse: 3.3647e-05 - val_loss: 2.7924e-05 - val_mse: 2.7924e-05\n",
      "Epoch 47/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.7080e-05 - mse: 2.7080e-05 - val_loss: 2.2431e-05 - val_mse: 2.2431e-05\n",
      "Epoch 48/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.1746e-05 - mse: 2.1746e-05 - val_loss: 1.8021e-05 - val_mse: 1.8021e-05\n",
      "Epoch 49/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.7470e-05 - mse: 1.7470e-05 - val_loss: 1.4465e-05 - val_mse: 1.4465e-05\n",
      "Epoch 50/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4026e-05 - mse: 1.4026e-05 - val_loss: 1.1637e-05 - val_mse: 1.1637e-05\n",
      "Epoch 51/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1281e-05 - mse: 1.1281e-05 - val_loss: 9.3579e-06 - val_mse: 9.3579e-06\n",
      "Epoch 52/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 9.0726e-06 - mse: 9.0726e-06 - val_loss: 7.5191e-06 - val_mse: 7.5191e-06\n",
      "Epoch 53/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.2920e-06 - mse: 7.2920e-06 - val_loss: 6.0405e-06 - val_mse: 6.0405e-06\n",
      "Epoch 54/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.8583e-06 - mse: 5.8583e-06 - val_loss: 4.8507e-06 - val_mse: 4.8507e-06\n",
      "Epoch 55/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.7070e-06 - mse: 4.7070e-06 - val_loss: 3.9061e-06 - val_mse: 3.9061e-06\n",
      "Epoch 56/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7910e-06 - mse: 3.7910e-06 - val_loss: 3.1386e-06 - val_mse: 3.1386e-06\n",
      "Epoch 57/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.0428e-06 - mse: 3.0428e-06 - val_loss: 2.5241e-06 - val_mse: 2.5241e-06\n",
      "Epoch 58/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.4480e-06 - mse: 2.4480e-06 - val_loss: 2.0331e-06 - val_mse: 2.0331e-06\n",
      "Epoch 59/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 1.9704e-06 - mse: 1.9704e-06 - val_loss: 1.6334e-06 - val_mse: 1.6334e-06\n",
      "Epoch 60/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.5827e-06 - mse: 1.5827e-06 - val_loss: 1.3110e-06 - val_mse: 1.3110e-06\n",
      "Epoch 61/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2705e-06 - mse: 1.2705e-06 - val_loss: 1.0549e-06 - val_mse: 1.0549e-06\n",
      "Epoch 62/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0222e-06 - mse: 1.0222e-06 - val_loss: 8.4713e-07 - val_mse: 8.4713e-07\n",
      "Epoch 63/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.2079e-07 - mse: 8.2079e-07 - val_loss: 6.8059e-07 - val_mse: 6.8059e-07\n",
      "Epoch 64/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.5968e-07 - mse: 6.5968e-07 - val_loss: 5.4813e-07 - val_mse: 5.4813e-07\n",
      "Epoch 65/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.3118e-07 - mse: 5.3118e-07 - val_loss: 4.4054e-07 - val_mse: 4.4054e-07\n",
      "Epoch 66/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.2689e-07 - mse: 4.2689e-07 - val_loss: 3.5425e-07 - val_mse: 3.5425e-07\n",
      "Epoch 67/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4356e-07 - mse: 3.4356e-07 - val_loss: 2.8497e-07 - val_mse: 2.8497e-07\n",
      "Epoch 68/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.7600e-07 - mse: 2.7600e-07 - val_loss: 2.2928e-07 - val_mse: 2.2928e-07\n",
      "Epoch 69/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.2207e-07 - mse: 2.2207e-07 - val_loss: 1.8429e-07 - val_mse: 1.8429e-07\n",
      "Epoch 70/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.7855e-07 - mse: 1.7855e-07 - val_loss: 1.4827e-07 - val_mse: 1.4827e-07\n",
      "Epoch 71/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4361e-07 - mse: 1.4361e-07 - val_loss: 1.1941e-07 - val_mse: 1.1941e-07\n",
      "Epoch 72/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1560e-07 - mse: 1.1560e-07 - val_loss: 9.5946e-08 - val_mse: 9.5946e-08\n",
      "Epoch 73/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.2881e-08 - mse: 9.2881e-08 - val_loss: 7.7089e-08 - val_mse: 7.7089e-08\n",
      "Epoch 74/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.4689e-08 - mse: 7.4689e-08 - val_loss: 6.2058e-08 - val_mse: 6.2058e-08\n",
      "Epoch 75/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.0120e-08 - mse: 6.0120e-08 - val_loss: 4.9852e-08 - val_mse: 4.9852e-08\n",
      "Epoch 76/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.8292e-08 - mse: 4.8292e-08 - val_loss: 4.0126e-08 - val_mse: 4.0126e-08\n",
      "Epoch 77/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.8866e-08 - mse: 3.8866e-08 - val_loss: 3.2288e-08 - val_mse: 3.2288e-08\n",
      "Epoch 78/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.1302e-08 - mse: 3.1302e-08 - val_loss: 2.6032e-08 - val_mse: 2.6032e-08\n",
      "Epoch 79/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 2.5240e-08 - mse: 2.5240e-08 - val_loss: 2.0938e-08 - val_mse: 2.0938e-08\n",
      "Epoch 80/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.0316e-08 - mse: 2.0316e-08 - val_loss: 1.6842e-08 - val_mse: 1.6842e-08\n",
      "Epoch 81/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.6333e-08 - mse: 1.6333e-08 - val_loss: 1.3572e-08 - val_mse: 1.3572e-08\n",
      "Epoch 82/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3179e-08 - mse: 1.3179e-08 - val_loss: 1.0933e-08 - val_mse: 1.0933e-08\n",
      "Epoch 83/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0598e-08 - mse: 1.0598e-08 - val_loss: 8.7828e-09 - val_mse: 8.7828e-09\n",
      "Epoch 84/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.5101e-09 - mse: 8.5101e-09 - val_loss: 7.0448e-09 - val_mse: 7.0448e-09\n",
      "Epoch 85/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.8353e-09 - mse: 6.8353e-09 - val_loss: 5.6844e-09 - val_mse: 5.6844e-09\n",
      "Epoch 86/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.5170e-09 - mse: 5.5170e-09 - val_loss: 4.5727e-09 - val_mse: 4.5727e-09\n",
      "Epoch 87/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4311e-09 - mse: 4.4311e-09 - val_loss: 3.6840e-09 - val_mse: 3.6840e-09\n",
      "Epoch 88/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.5791e-09 - mse: 3.5791e-09 - val_loss: 2.9721e-09 - val_mse: 2.9721e-09\n",
      "Epoch 89/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.8786e-09 - mse: 2.8786e-09 - val_loss: 2.3822e-09 - val_mse: 2.3822e-09\n",
      "Epoch 90/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.3070e-09 - mse: 2.3070e-09 - val_loss: 1.9106e-09 - val_mse: 1.9106e-09\n",
      "Epoch 91/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.8575e-09 - mse: 1.8575e-09 - val_loss: 1.5459e-09 - val_mse: 1.5459e-09\n",
      "Epoch 92/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.5014e-09 - mse: 1.5014e-09 - val_loss: 1.2404e-09 - val_mse: 1.2404e-09\n",
      "Epoch 93/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2049e-09 - mse: 1.2049e-09 - val_loss: 1.0028e-09 - val_mse: 1.0028e-09\n",
      "Epoch 94/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 9.7256e-10 - mse: 9.7256e-10 - val_loss: 8.0902e-10 - val_mse: 8.0902e-10\n",
      "Epoch 95/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.8522e-10 - mse: 7.8522e-10 - val_loss: 6.4927e-10 - val_mse: 6.4927e-10\n",
      "Epoch 96/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 6.2683e-10 - mse: 6.2683e-10 - val_loss: 5.2272e-10 - val_mse: 5.2272e-10\n",
      "Epoch 97/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.0989e-10 - mse: 5.0989e-10 - val_loss: 4.2574e-10 - val_mse: 4.2574e-10\n",
      "Epoch 98/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1251e-10 - mse: 4.1251e-10 - val_loss: 3.3903e-10 - val_mse: 3.3903e-10\n",
      "Epoch 99/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 3.2904e-10 - mse: 3.2904e-10 - val_loss: 2.7121e-10 - val_mse: 2.7121e-10\n",
      "Epoch 100/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.6253e-10 - mse: 2.6253e-10 - val_loss: 2.1886e-10 - val_mse: 2.1886e-10\n",
      "Epoch 101/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.1181e-10 - mse: 2.1181e-10 - val_loss: 1.7757e-10 - val_mse: 1.7757e-10\n",
      "Epoch 102/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.7362e-10 - mse: 1.7362e-10 - val_loss: 1.4655e-10 - val_mse: 1.4655e-10\n",
      "Epoch 103/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4418e-10 - mse: 1.4418e-10 - val_loss: 1.2164e-10 - val_mse: 1.2164e-10\n",
      "Epoch 104/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1880e-10 - mse: 1.1880e-10 - val_loss: 9.9259e-11 - val_mse: 9.9259e-11\n",
      "Epoch 105/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 9.6172e-11 - mse: 9.6172e-11 - val_loss: 7.8836e-11 - val_mse: 7.8836e-11\n",
      "Epoch 106/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.4981e-11 - mse: 7.4981e-11 - val_loss: 6.0495e-11 - val_mse: 6.0495e-11\n",
      "Epoch 107/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.7180e-11 - mse: 5.7180e-11 - val_loss: 4.5298e-11 - val_mse: 4.5298e-11\n",
      "Epoch 108/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 4.1417e-11 - mse: 4.1417e-11 - val_loss: 3.1992e-11 - val_mse: 3.1992e-11\n",
      "Epoch 109/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.9712e-11 - mse: 2.9712e-11 - val_loss: 2.4000e-11 - val_mse: 2.4000e-11\n",
      "Epoch 110/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.3772e-11 - mse: 2.3772e-11 - val_loss: 2.0242e-11 - val_mse: 2.0242e-11\n",
      "Epoch 111/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 2.0413e-11 - mse: 2.0413e-11 - val_loss: 1.7955e-11 - val_mse: 1.7955e-11\n",
      "Epoch 112/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.8154e-11 - mse: 1.8154e-11 - val_loss: 1.6833e-11 - val_mse: 1.6833e-11\n",
      "Epoch 113/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.7896e-11 - mse: 1.7896e-11 - val_loss: 1.6833e-11 - val_mse: 1.6833e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.7608e-11 - mse: 1.7608e-11 - val_loss: 1.6277e-11 - val_mse: 1.6277e-11\n",
      "Epoch 115/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.7269e-11 - mse: 1.7269e-11 - val_loss: 1.5663e-11 - val_mse: 1.5663e-11\n",
      "Epoch 116/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.6618e-11 - mse: 1.6618e-11 - val_loss: 1.5199e-11 - val_mse: 1.5199e-11\n",
      "Epoch 117/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.5911e-11 - mse: 1.5911e-11 - val_loss: 1.4704e-11 - val_mse: 1.4704e-11\n",
      "Epoch 118/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.5680e-11 - mse: 1.5680e-11 - val_loss: 1.4704e-11 - val_mse: 1.4704e-11\n",
      "Epoch 119/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.5299e-11 - mse: 1.5299e-11 - val_loss: 1.4116e-11 - val_mse: 1.4116e-11\n",
      "Epoch 120/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.5120e-11 - mse: 1.5120e-11 - val_loss: 1.4116e-11 - val_mse: 1.4116e-11\n",
      "Epoch 121/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.5165e-11 - mse: 1.5165e-11 - val_loss: 1.4195e-11 - val_mse: 1.4195e-11\n",
      "Epoch 122/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.5125e-11 - mse: 1.5125e-11 - val_loss: 1.4116e-11 - val_mse: 1.4116e-11\n",
      "Epoch 123/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4690e-11 - mse: 1.4690e-11 - val_loss: 1.3623e-11 - val_mse: 1.3623e-11\n",
      "Epoch 124/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4567e-11 - mse: 1.4567e-11 - val_loss: 1.3623e-11 - val_mse: 1.3623e-11\n",
      "Epoch 125/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4567e-11 - mse: 1.4567e-11 - val_loss: 1.3623e-11 - val_mse: 1.3623e-11\n",
      "Epoch 126/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4498e-11 - mse: 1.4498e-11 - val_loss: 1.3166e-11 - val_mse: 1.3166e-11\n",
      "Epoch 127/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4050e-11 - mse: 1.4050e-11 - val_loss: 1.3166e-11 - val_mse: 1.3166e-11\n",
      "Epoch 128/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4050e-11 - mse: 1.4050e-11 - val_loss: 1.3166e-11 - val_mse: 1.3166e-11\n",
      "Epoch 129/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.4050e-11 - mse: 1.4050e-11 - val_loss: 1.3166e-11 - val_mse: 1.3166e-11\n",
      "Epoch 130/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3675e-11 - mse: 1.3675e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 131/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3508e-11 - mse: 1.3508e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 132/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3508e-11 - mse: 1.3508e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 133/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3517e-11 - mse: 1.3517e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 134/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3508e-11 - mse: 1.3508e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 135/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3508e-11 - mse: 1.3508e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 136/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3508e-11 - mse: 1.3508e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 137/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3508e-11 - mse: 1.3508e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 138/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3508e-11 - mse: 1.3508e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 139/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3508e-11 - mse: 1.3508e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 140/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3508e-11 - mse: 1.3508e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 141/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3508e-11 - mse: 1.3508e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 142/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3533e-11 - mse: 1.3533e-11 - val_loss: 1.2770e-11 - val_mse: 1.2770e-11\n",
      "Epoch 143/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3536e-11 - mse: 1.3536e-11 - val_loss: 1.2789e-11 - val_mse: 1.2789e-11\n",
      "Epoch 144/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3539e-11 - mse: 1.3539e-11 - val_loss: 1.2789e-11 - val_mse: 1.2789e-11\n",
      "Epoch 145/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3539e-11 - mse: 1.3539e-11 - val_loss: 1.2789e-11 - val_mse: 1.2789e-11\n",
      "Epoch 146/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3539e-11 - mse: 1.3539e-11 - val_loss: 1.2789e-11 - val_mse: 1.2789e-11\n",
      "Epoch 147/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3539e-11 - mse: 1.3539e-11 - val_loss: 1.2789e-11 - val_mse: 1.2789e-11\n",
      "Epoch 148/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3543e-11 - mse: 1.3543e-11 - val_loss: 1.2770e-11 - val_mse: 1.2770e-11\n",
      "Epoch 149/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3526e-11 - mse: 1.3526e-11 - val_loss: 1.2770e-11 - val_mse: 1.2770e-11\n",
      "Epoch 150/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3526e-11 - mse: 1.3526e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 151/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3508e-11 - mse: 1.3508e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 152/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3508e-11 - mse: 1.3508e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 153/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3508e-11 - mse: 1.3508e-11 - val_loss: 1.2689e-11 - val_mse: 1.2689e-11\n",
      "Epoch 154/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3540e-11 - mse: 1.3540e-11 - val_loss: 1.2770e-11 - val_mse: 1.2770e-11\n",
      "Epoch 155/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3526e-11 - mse: 1.3526e-11 - val_loss: 1.2210e-11 - val_mse: 1.2210e-11\n",
      "Epoch 156/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3083e-11 - mse: 1.3083e-11 - val_loss: 1.2240e-11 - val_mse: 1.2240e-11\n",
      "Epoch 157/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3056e-11 - mse: 1.3056e-11 - val_loss: 1.2210e-11 - val_mse: 1.2210e-11\n",
      "Epoch 158/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3035e-11 - mse: 1.3035e-11 - val_loss: 1.2210e-11 - val_mse: 1.2210e-11\n",
      "Epoch 159/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3035e-11 - mse: 1.3035e-11 - val_loss: 1.2210e-11 - val_mse: 1.2210e-11\n",
      "Epoch 160/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3035e-11 - mse: 1.3035e-11 - val_loss: 1.2210e-11 - val_mse: 1.2210e-11\n",
      "Epoch 161/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3035e-11 - mse: 1.3035e-11 - val_loss: 1.2210e-11 - val_mse: 1.2210e-11\n",
      "Epoch 162/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.3035e-11 - mse: 1.3035e-11 - val_loss: 1.2210e-11 - val_mse: 1.2210e-11\n",
      "Epoch 163/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3035e-11 - mse: 1.3035e-11 - val_loss: 1.2210e-11 - val_mse: 1.2210e-11\n",
      "Epoch 164/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3035e-11 - mse: 1.3035e-11 - val_loss: 1.2210e-11 - val_mse: 1.2210e-11\n",
      "Epoch 165/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3035e-11 - mse: 1.3035e-11 - val_loss: 1.2252e-11 - val_mse: 1.2252e-11\n",
      "Epoch 166/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3039e-11 - mse: 1.3039e-11 - val_loss: 1.2252e-11 - val_mse: 1.2252e-11\n",
      "Epoch 167/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3039e-11 - mse: 1.3039e-11 - val_loss: 1.2252e-11 - val_mse: 1.2252e-11\n",
      "Epoch 168/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3039e-11 - mse: 1.3039e-11 - val_loss: 1.2252e-11 - val_mse: 1.2252e-11\n",
      "Epoch 169/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.3039e-11 - mse: 1.3039e-11 - val_loss: 1.2252e-11 - val_mse: 1.2252e-11\n",
      "Epoch 170/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2821e-11 - mse: 1.2821e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 171/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2563e-11 - mse: 1.2563e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 172/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 173/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 174/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 175/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 176/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 177/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 178/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 179/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 180/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 181/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 182/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 183/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 184/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 185/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2557e-11 - mse: 1.2557e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 186/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 187/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2553e-11 - mse: 1.2553e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n",
      "Epoch 188/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2564e-11 - mse: 1.2564e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 189/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 190/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 191/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n",
      "Epoch 192/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2510e-11 - mse: 1.2510e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n",
      "Epoch 193/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2510e-11 - mse: 1.2510e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n",
      "Epoch 194/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2542e-11 - mse: 1.2542e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 195/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2521e-11 - mse: 1.2521e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 196/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2521e-11 - mse: 1.2521e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 197/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2528e-11 - mse: 1.2528e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n",
      "Epoch 198/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2510e-11 - mse: 1.2510e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 199/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 200/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 201/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2572e-11 - mse: 1.2572e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 202/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2566e-11 - mse: 1.2566e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 203/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 204/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 205/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 206/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 207/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 208/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 209/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 210/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2571e-11 - mse: 1.2571e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 211/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2530e-11 - mse: 1.2530e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 212/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2521e-11 - mse: 1.2521e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 213/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2521e-11 - mse: 1.2521e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 214/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2521e-11 - mse: 1.2521e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 215/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2521e-11 - mse: 1.2521e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 216/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2521e-11 - mse: 1.2521e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 217/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2542e-11 - mse: 1.2542e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n",
      "Epoch 218/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2541e-11 - mse: 1.2541e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n",
      "Epoch 219/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2510e-11 - mse: 1.2510e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n",
      "Epoch 220/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2510e-11 - mse: 1.2510e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n",
      "Epoch 221/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2510e-11 - mse: 1.2510e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n",
      "Epoch 222/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2510e-11 - mse: 1.2510e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2510e-11 - mse: 1.2510e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n",
      "Epoch 224/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.2535e-11 - mse: 1.2535e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 225/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2521e-11 - mse: 1.2521e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 226/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2521e-11 - mse: 1.2521e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 227/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2521e-11 - mse: 1.2521e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 228/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2521e-11 - mse: 1.2521e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 229/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2521e-11 - mse: 1.2521e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n",
      "Epoch 230/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2510e-11 - mse: 1.2510e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n",
      "Epoch 231/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2516e-11 - mse: 1.2516e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 232/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 233/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 234/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 235/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 236/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 237/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 238/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2551e-11 - mse: 1.2551e-11 - val_loss: 1.1729e-11 - val_mse: 1.1729e-11\n",
      "Epoch 239/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2570e-11 - mse: 1.2570e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 240/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 241/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2567e-11 - mse: 1.2567e-11 - val_loss: 1.1673e-11 - val_mse: 1.1673e-11\n",
      "Epoch 242/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2562e-11 - mse: 1.2562e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n",
      "Epoch 243/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2510e-11 - mse: 1.2510e-11 - val_loss: 1.1779e-11 - val_mse: 1.1779e-11\n",
      "Epoch 244/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2535e-11 - mse: 1.2535e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 245/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2521e-11 - mse: 1.2521e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 246/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2521e-11 - mse: 1.2521e-11 - val_loss: 1.1762e-11 - val_mse: 1.1762e-11\n",
      "Epoch 247/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.2401e-11 - mse: 1.2401e-11 - val_loss: 1.1392e-11 - val_mse: 1.1392e-11\n",
      "Epoch 248/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1991e-11 - mse: 1.1991e-11 - val_loss: 1.1392e-11 - val_mse: 1.1392e-11\n",
      "Epoch 249/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1991e-11 - mse: 1.1991e-11 - val_loss: 1.1392e-11 - val_mse: 1.1392e-11\n",
      "Epoch 250/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1678e-11 - mse: 1.1678e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 251/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 252/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 253/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 254/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 255/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 256/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 257/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 258/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1514e-11 - mse: 1.1514e-11 - val_loss: 1.0895e-11 - val_mse: 1.0895e-11\n",
      "Epoch 259/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1547e-11 - mse: 1.1547e-11 - val_loss: 1.0895e-11 - val_mse: 1.0895e-11\n",
      "Epoch 260/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1547e-11 - mse: 1.1547e-11 - val_loss: 1.0895e-11 - val_mse: 1.0895e-11\n",
      "Epoch 261/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1547e-11 - mse: 1.1547e-11 - val_loss: 1.0895e-11 - val_mse: 1.0895e-11\n",
      "Epoch 262/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1547e-11 - mse: 1.1547e-11 - val_loss: 1.0895e-11 - val_mse: 1.0895e-11\n",
      "Epoch 263/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1547e-11 - mse: 1.1547e-11 - val_loss: 1.0895e-11 - val_mse: 1.0895e-11\n",
      "Epoch 264/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1547e-11 - mse: 1.1547e-11 - val_loss: 1.0895e-11 - val_mse: 1.0895e-11\n",
      "Epoch 265/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1547e-11 - mse: 1.1547e-11 - val_loss: 1.0895e-11 - val_mse: 1.0895e-11\n",
      "Epoch 266/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1531e-11 - mse: 1.1531e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 267/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 268/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 269/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 270/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 271/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 272/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0895e-11 - val_mse: 1.0895e-11\n",
      "Epoch 273/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1547e-11 - mse: 1.1547e-11 - val_loss: 1.0895e-11 - val_mse: 1.0895e-11\n",
      "Epoch 274/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1547e-11 - mse: 1.1547e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 275/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 276/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 277/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 278/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 279/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 280/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 281/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1514e-11 - mse: 1.1514e-11 - val_loss: 1.0895e-11 - val_mse: 1.0895e-11\n",
      "Epoch 282/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1547e-11 - mse: 1.1547e-11 - val_loss: 1.0895e-11 - val_mse: 1.0895e-11\n",
      "Epoch 283/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1542e-11 - mse: 1.1542e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 284/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0997e-11 - val_mse: 1.0997e-11\n",
      "Epoch 285/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1507e-11 - mse: 1.1507e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 286/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 287/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 288/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 289/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 290/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 291/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 292/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 293/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 294/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 295/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 296/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 297/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 298/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 299/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 300/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 301/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 302/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 303/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 304/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 305/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 306/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 307/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1133e-11 - mse: 1.1133e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 308/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 309/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 310/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 311/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 312/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 313/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 314/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 315/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 316/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 317/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 318/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 319/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 320/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1108e-11 - mse: 1.1108e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 321/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 322/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 323/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 324/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 325/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 326/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 327/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 328/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 329/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 330/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 331/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 333/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 334/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 335/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 336/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 337/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 338/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 339/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 340/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 341/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 342/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 343/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 344/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 345/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 346/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 347/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 348/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 349/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 350/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 351/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 352/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 353/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 354/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1074e-11 - mse: 1.1074e-11 - val_loss: 1.0451e-11 - val_mse: 1.0451e-11\n",
      "Epoch 355/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1060e-11 - mse: 1.1060e-11 - val_loss: 1.0451e-11 - val_mse: 1.0451e-11\n",
      "Epoch 356/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1060e-11 - mse: 1.1060e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 357/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 358/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 359/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 360/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 361/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 362/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 363/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 364/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 365/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 366/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 367/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 368/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 369/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 370/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 371/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 372/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 373/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 374/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 375/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 376/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 377/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 378/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 379/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 380/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 381/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 382/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 383/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 384/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 385/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 386/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 387/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 388/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 389/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 390/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 391/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 392/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 393/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 394/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 395/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 396/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 397/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 398/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 399/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 400/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 401/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 402/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 403/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 404/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 405/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 406/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 407/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 408/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 409/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 410/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 411/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 412/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 413/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 414/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 415/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1115e-11 - mse: 1.1115e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 416/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 417/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 418/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 419/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 420/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 421/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 422/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 423/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 424/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 425/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1097e-11 - mse: 1.1097e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 426/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 427/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 428/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 429/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 430/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 431/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 432/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 433/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 434/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 435/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 436/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 437/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1120e-11 - mse: 1.1120e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 438/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 439/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 440/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 441/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 442/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 443/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 444/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1083e-11 - mse: 1.1083e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 445/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 446/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 447/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 448/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 449/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 450/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 451/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 452/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 453/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 454/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 455/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1121e-11 - mse: 1.1121e-11 - val_loss: 1.0377e-11 - val_mse: 1.0377e-11\n",
      "Epoch 456/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1114e-11 - mse: 1.1114e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 457/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 458/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 459/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 460/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 461/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 462/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 463/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 464/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 465/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 466/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 467/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 468/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 469/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 470/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 471/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0427e-11 - val_mse: 1.0427e-11\n",
      "Epoch 472/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.1086e-11 - mse: 1.1086e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 473/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 474/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 475/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 476/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 477/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 478/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 479/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 480/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 481/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 482/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 483/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 484/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 485/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 486/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 487/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 488/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 489/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 490/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 491/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 492/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 493/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 494/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0680e-11 - mse: 1.0680e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 495/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 496/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 497/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 498/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 499/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 500/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 501/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 502/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 503/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 504/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 505/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 506/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 507/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 508/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 509/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 510/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 511/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 512/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 513/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 514/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 515/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 516/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 517/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 518/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 519/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 520/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 521/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 522/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0664e-11 - mse: 1.0664e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 523/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 524/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 525/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 526/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 527/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 528/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 529/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 530/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 531/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 532/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 533/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 534/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 535/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 536/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 537/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 538/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0670e-11 - mse: 1.0670e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 539/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 540/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 541/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 542/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0650e-11 - mse: 1.0650e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 543/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 544/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 545/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0679e-11 - mse: 1.0679e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 546/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 547/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 548/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 549/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 550/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 551/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 552/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 553/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 554/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 555/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 556/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 557/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0676e-11 - mse: 1.0676e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 558/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 559/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 560/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 561/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 562/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 563/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 564/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 565/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 566/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 567/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 568/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 569/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0673e-11 - mse: 1.0673e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 570/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 571/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 572/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 573/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 574/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 575/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 576/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 577/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 578/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 579/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 580/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 581/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 582/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 583/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0652e-11 - mse: 1.0652e-11 - val_loss: 9.9712e-12 - val_mse: 9.9712e-12\n",
      "Epoch 584/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0645e-11 - mse: 1.0645e-11 - val_loss: 9.9712e-12 - val_mse: 9.9712e-12\n",
      "Epoch 585/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0645e-11 - mse: 1.0645e-11 - val_loss: 9.9712e-12 - val_mse: 9.9712e-12\n",
      "Epoch 586/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0645e-11 - mse: 1.0645e-11 - val_loss: 9.9712e-12 - val_mse: 9.9712e-12\n",
      "Epoch 587/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0645e-11 - mse: 1.0645e-11 - val_loss: 9.9712e-12 - val_mse: 9.9712e-12\n",
      "Epoch 588/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0645e-11 - mse: 1.0645e-11 - val_loss: 9.9712e-12 - val_mse: 9.9712e-12\n",
      "Epoch 589/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0645e-11 - mse: 1.0645e-11 - val_loss: 1.0049e-11 - val_mse: 1.0049e-11\n",
      "Epoch 590/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0629e-11 - mse: 1.0629e-11 - val_loss: 1.0049e-11 - val_mse: 1.0049e-11\n",
      "Epoch 591/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0629e-11 - mse: 1.0629e-11 - val_loss: 1.0049e-11 - val_mse: 1.0049e-11\n",
      "Epoch 592/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0629e-11 - mse: 1.0629e-11 - val_loss: 1.0049e-11 - val_mse: 1.0049e-11\n",
      "Epoch 593/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0629e-11 - mse: 1.0629e-11 - val_loss: 1.0049e-11 - val_mse: 1.0049e-11\n",
      "Epoch 594/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0631e-11 - mse: 1.0631e-11 - val_loss: 9.9712e-12 - val_mse: 9.9712e-12\n",
      "Epoch 595/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0650e-11 - mse: 1.0650e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 596/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 597/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 598/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 599/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 600/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 601/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 602/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 603/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 604/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 605/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 606/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 607/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 608/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 609/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 610/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 611/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 612/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 613/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0634e-11 - mse: 1.0634e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 614/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 615/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0683e-11 - mse: 1.0683e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 616/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 617/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 618/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 619/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 620/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 621/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 622/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 623/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 624/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 625/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 626/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 627/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 628/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 629/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 630/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 631/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 632/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 633/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0695e-11 - mse: 1.0695e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 634/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 635/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 636/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 637/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 638/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 639/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 640/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 641/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 642/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 643/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 644/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 645/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 646/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 647/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 648/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 649/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 650/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0695e-11 - mse: 1.0695e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 651/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 652/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 653/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 654/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 655/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 656/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 657/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 658/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 659/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0686e-11 - mse: 1.0686e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 660/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 661/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 662/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0647e-11 - mse: 1.0647e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 663/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 664/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 665/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 666/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 667/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0662e-11 - mse: 1.0662e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 668/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 669/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 670/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 671/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0670e-11 - mse: 1.0670e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 672/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0664e-11 - mse: 1.0664e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 673/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 674/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 675/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 676/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 677/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 678/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 679/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 680/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 681/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 682/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 683/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0667e-11 - mse: 1.0667e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 684/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 685/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 686/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 687/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 688/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 689/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 690/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0652e-11 - mse: 1.0652e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 691/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 692/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0645e-11 - mse: 1.0645e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 693/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 694/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 695/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 696/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 697/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 698/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0667e-11 - mse: 1.0667e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 699/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 700/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 701/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 702/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 703/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 704/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 705/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 706/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 707/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 708/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 709/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 710/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 711/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 712/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 713/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 714/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 715/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 716/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 717/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 718/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 719/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 720/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 721/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 722/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 723/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 724/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 725/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 726/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 727/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 728/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 729/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 730/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 731/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 732/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 733/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 734/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 735/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 736/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 737/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 738/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0662e-11 - mse: 1.0662e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 739/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 740/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 741/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 742/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 743/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 744/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 745/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 746/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 747/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0654e-11 - mse: 1.0654e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 748/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 749/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 750/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 751/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 752/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0689e-11 - mse: 1.0689e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 753/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 754/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 755/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 756/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 757/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 758/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 759/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 760/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 761/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 762/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 763/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 764/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 765/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 766/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 767/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 768/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 769/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 770/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 771/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 772/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0673e-11 - mse: 1.0673e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 773/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 774/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 775/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 776/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0678e-11 - mse: 1.0678e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 777/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 778/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 779/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 780/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 781/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 782/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 783/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 784/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 785/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 786/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 787/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 788/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 789/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 790/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 791/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 792/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 793/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 794/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 795/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 796/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 797/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 798/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 799/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 800/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 801/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 802/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 803/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 804/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 805/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 806/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 807/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 9.9470e-12 - val_mse: 9.9470e-12\n",
      "Epoch 808/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0698e-11 - mse: 1.0698e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 809/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 810/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 811/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 812/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 813/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 814/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 815/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 816/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 817/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 818/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0673e-11 - mse: 1.0673e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 819/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 820/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 821/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 822/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 823/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 824/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 825/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 826/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 827/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 828/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 829/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 830/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 831/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 832/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 833/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 834/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0682e-11 - mse: 1.0682e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 835/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 836/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0652e-11 - mse: 1.0652e-11 - val_loss: 9.9712e-12 - val_mse: 9.9712e-12\n",
      "Epoch 837/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0645e-11 - mse: 1.0645e-11 - val_loss: 9.9712e-12 - val_mse: 9.9712e-12\n",
      "Epoch 838/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0645e-11 - mse: 1.0645e-11 - val_loss: 9.9712e-12 - val_mse: 9.9712e-12\n",
      "Epoch 839/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0645e-11 - mse: 1.0645e-11 - val_loss: 9.9712e-12 - val_mse: 9.9712e-12\n",
      "Epoch 840/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0645e-11 - mse: 1.0645e-11 - val_loss: 9.9712e-12 - val_mse: 9.9712e-12\n",
      "Epoch 841/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0645e-11 - mse: 1.0645e-11 - val_loss: 9.9712e-12 - val_mse: 9.9712e-12\n",
      "Epoch 842/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0646e-11 - mse: 1.0646e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 843/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0650e-11 - mse: 1.0650e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 844/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0644e-11 - mse: 1.0644e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 845/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0651e-11 - mse: 1.0651e-11 - val_loss: 1.0043e-11 - val_mse: 1.0043e-11\n",
      "Epoch 846/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0664e-11 - mse: 1.0664e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 847/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 848/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 849/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 850/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 851/5000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 852/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 853/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 854/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 855/5000\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 1.0655e-11 - mse: 1.0655e-11 - val_loss: 1.0010e-11 - val_mse: 1.0010e-11\n",
      "Epoch 856/5000\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 7.7136e-12 - mse: 7.7136e-12"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-24bdd4b22aa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# a good idea is to shuffle input before at each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           validation_data=(x_valid, y_valid))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1134\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1371\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec, job_token)\u001b[0m\n\u001b[1;32m    694\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    720\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m    721\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_job_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         gen_experimental_dataset_ops.make_data_service_iterator(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3005\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   3006\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MakeIterator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3007\u001b[0;31m         tld.op_callbacks, dataset, iterator)\n\u001b[0m\u001b[1;32m   3008\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3009\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit the model using training dataset\n",
    "# over 10 epochs of 32 batch size each\n",
    "# report training progress against validation data\n",
    "history = model.fit(x=x_train, y=y_train, \n",
    "          batch_size=32, epochs=50,\n",
    "          shuffle=True, # a good idea is to shuffle input before at each epoch\n",
    "          validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at weights and biases we can see if the linear fit was successfull: $w_1$ represents the angular coefficient, $b$ the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return weights and biases\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(x_valid, y_valid, batch_size=32, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model with the exact curve\n",
    "score = model.evaluate(x_valid, y_target, batch_size=32, verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into training history\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_predicted = np.random.uniform(-1, 1, 100)\n",
    "y_predicted = model.predict(x_predicted)\n",
    "plt.scatter(x_predicted, y_predicted,color='r')\n",
    "plt.plot(x_valid, y_target)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.1\n",
    "\n",
    "In order to make practice with NN, explore how does the previous linear regression depend on the number of epochs, $N_{\\mathrm{epochs}}$, the number of data points $N_{\\mathrm{train}}$ and on the noise $\\sigma$. Try to improve the previous result operating on these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.2\n",
    "\n",
    "Try to extend the model to obtain a reasonable fit of the following polynomial of order 3:\n",
    "\n",
    "$$\n",
    "f(x)=4-3x-2x^2+3x^3\n",
    "$$\n",
    "for $x \\in [-1,1]$.\n",
    "\n",
    "In order to make practice with NN, explore reasonable different choices for:\n",
    "\n",
    "- the number of layers\n",
    "- the number of neurons in each layer\n",
    "- the activation function\n",
    "- the optimizer\n",
    "- the loss function\n",
    "  \n",
    "Make graphs comparing fits for different NNs.\n",
    "Check your NN models by seeing how well your fits predict newly generated test data (including on data outside the range you fit. How well do your NN do on points in the range of $x$ where you trained the model? How about points outside the original training data set? \n",
    "Summarize what you have learned about the relationship between model complexity (number of parameters), goodness of fit on training data, and the ability to predict well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.3\n",
    "  \n",
    "Try to extend the model to fit a simple trigonometric 2D function such as $f(x,y) = \\sin(x^2+y^2)$ in the range $x \\in [-3/2,3/2]$ and $y \\in [-3/2,3/2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas to meditate on these exercises and judge your results can be found <a href=https://xkcd.com/2048/>here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Supplementary material: Keras model.fit available callbacks</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .fit method can also get <a href=https://keras.io/callbacks/>callback</a> functions which can be used to customize the fitting procedure with special actions.\n",
    "\n",
    "Keras provides some predefined callbacks to feed in, among them for example:\n",
    "- **TerminateOnNaN()**: that terminates training when a NaN loss is encountered\n",
    "- **ModelCheckpoint(filepath)**: that save the model after every epoch\n",
    "- **EarlyStopping()**: which stop training when a monitored quantity has stopped improving\n",
    "\n",
    "You can select one or more callback and pass them as a list to the callback argument of the fit method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to construct a callback object to represent how estimated parameters are converging during the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "class PlotCurrentEstimate(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_valid, y_valid):\n",
    "        \"\"\"Keras Callback which plot current model estimate against reference target\"\"\"\n",
    "        \n",
    "        # convert numpy arrays into lists for plotting purposes\n",
    "        self.x_valid = list(x_valid[:])\n",
    "        self.y_valid = list(y_valid[:])\n",
    "        self.iter=0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        temp = self.model.predict(self.x_valid, batch_size=None, verbose=False, steps=None)\n",
    "        self.y_curr = list(temp[:]) # convert numpy array into list\n",
    "        \n",
    "        self.iter+=1\n",
    "        if self.iter%10 == 0:\n",
    "            clear_output(wait=True)            \n",
    "            self.eplot = plt.subplot(1,1,1)\n",
    "            self.eplot.clear()     \n",
    "            self.eplot.scatter(self.x_valid, self.y_curr, color=\"blue\", s=4, marker=\"o\", label=\"estimate\")\n",
    "            self.eplot.scatter(self.x_valid, self.y_valid, color=\"red\", s=4, marker=\"x\", label=\"valid\")\n",
    "            self.eplot.legend()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use also an EarlyStopping callback on the val_loss quantity. This will stop the training process as soon as the val_loss quantity does not improve anymore after an amount of epochs, preventing a long time of wated computation to take over without useful results.\n",
    "\n",
    "<code>keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)</code>\n",
    "\n",
    "Arguments:\n",
    "\n",
    "- <code>monitor</code>: quantity to be monitored. \n",
    "- <code>min_delta:</code> minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. \n",
    "- <code>patience:</code> number of epochs with no improvement after which training will be stopped. \n",
    "- <code>verbose:</code> verbosity mode. \n",
    "- <code>mode:</code> one of {auto, min, max}. In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. \n",
    "- <code>baseline:</code> Baseline value for the monitored quantity to reach. Training will stop if the model doesn't show improvement over the baseline. \n",
    "- <code>restore_best_weights:</code> whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "50/50 [==============================] - 0s 8ms/step - loss: 6.0297e-06 - mean_squared_error: 6.0297e-06 - val_loss: 5.9017e-06 - val_mean_squared_error: 5.9017e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[1.9956843]], dtype=float32), array([0.9996678], dtype=float32)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_estimate = PlotCurrentEstimate(x_valid, y_valid)\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                  min_delta=0, patience=100, mode='auto')\n",
    "\n",
    "model.fit(x_valid, y_valid, batch_size=32, epochs=150,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          callbacks=[ plot_estimate, earlystop]\n",
    "          )\n",
    "\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
